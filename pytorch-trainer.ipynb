{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"datasets/hce_full_train_small.csv\"\n",
    "EVAL_COLUMN = \"eval_8\"\n",
    "FLAG_COLUMN = None\n",
    "FIX_MATE_ERROR = True\n",
    "EVAL_MAX = 100000\n",
    "ACCEPTABLE_MATE_DEPTH = 0\n",
    "\n",
    "\n",
    "MICRO_BATCH_SIZE = 512\n",
    "TRAIN_SPLIT = 0.80\n",
    "NUM_WORKERS = 5\n",
    "TRAIN_EPOCHS = 40\n",
    "REPORT_FREQ = 500\n",
    "\n",
    "\n",
    "\n",
    "INPUT_SIZE = 768\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"device = \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FenDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Assumes a small dataset => loads everything to memory\n",
    "    FAR from clean but works :) \n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, eval_column):\n",
    "        # Load data\n",
    "        df = pd.read_csv(csv_file, dtype={\"best_move\": \"string\"})\n",
    "        \n",
    "        # Remove mate positions\n",
    "        if FIX_MATE_ERROR:\n",
    "            for i in tqdm(range(2, 9), \"Fixing mate errors in dataset\"):\n",
    "                prev = f\"eval_{i - 1}\"\n",
    "                now = f\"eval_{i}\"\n",
    "                df.loc[(df[prev] == EVAL_MAX) | (df[prev] == -EVAL_MAX), now] = df[prev]\n",
    "\n",
    "        # Remove captures\n",
    "        if FLAG_COLUMN:\n",
    "            raw_n = len(df)\n",
    "            df = df[df[FLAG_COLUMN] % 2 == 0]\n",
    "            print(\"Removed\", raw_n - len(df), \"captures\")\n",
    "\n",
    "        # Remove mate positions\n",
    "        print(\"Removing mates\")\n",
    "        mate_in = np.full(len(df.index), 1000)\n",
    "        for i in range(1, 9):\n",
    "            is_mate = (df[f\"eval_{i}\"].abs() == EVAL_MAX)\n",
    "            mate_in[is_mate] = np.minimum(mate_in[is_mate], i)\n",
    "            \n",
    "        df[\"mate_in\"] = mate_in\n",
    "        df = df[(df[\"mate_in\"] <= ACCEPTABLE_MATE_DEPTH) | (df[\"mate_in\"] == 1000)]\n",
    "\n",
    "        # Clip eval values\n",
    "        df[eval_column] = df[eval_column].clip(lower=-10000, upper=10000)\n",
    "\n",
    "        # centipawns to pawns\n",
    "        df[eval_column] = df[eval_column] / 100\n",
    "\n",
    "        # Safety check\n",
    "        df = df[~df[eval_column].isna()]\n",
    "        \n",
    "        # Parse fen\n",
    "        print(\"Parsing fens to stm\")\n",
    "        df[\"stm\"] = df[\"fen\"].progress_apply(FenDataset.fen_to_side_to_move)\n",
    "\n",
    "        print(\"Parsing fens to features\")\n",
    "        df[\"pieces\"] = df[\"fen\"].progress_apply(FenDataset.fen_to_features)\n",
    "\n",
    "        # Transfrom to list for faster access (not sure if faster tbh)\n",
    "        self._dataset_py = list(df[[\"pieces\", eval_column, \"stm\"]].to_records(index=False))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset_py)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self._dataset_py[idx][0]\n",
    "\n",
    "        # Multi hot encode\n",
    "        white_features = np.zeros(INPUT_SIZE)\n",
    "        black_features = np.zeros(INPUT_SIZE)\n",
    "        white_features[features[0]] = 1\n",
    "        black_features[features[1]] = 1\n",
    "\n",
    "        evaluation = self._dataset_py[idx][1]\n",
    "        side_to_move = self._dataset_py[idx][2]\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(white_features).to(torch.float32), \n",
    "            torch.from_numpy(black_features).to(torch.float32), \n",
    "            torch.tensor([side_to_move]).to(torch.float32),\n",
    "            torch.tensor([evaluation]).to(torch.float32)   \n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def fen_to_features(fen_str):\n",
    "        features_white = []\n",
    "        features_black = []\n",
    "        fen_parts = fen_str.split()\n",
    "        rank = 7\n",
    "        file = 0\n",
    "        PIECES = \"pnbrqk\"\n",
    "\n",
    "        for char in fen_parts[0]:\n",
    "            if char == '/':\n",
    "                rank -= 1\n",
    "                file = 0\n",
    "            elif char.isdigit():\n",
    "                file += int(char)\n",
    "            else:\n",
    "                square = rank * 8 + file\n",
    "                piece_id = PIECES.find(char.lower())\n",
    "                color = 1 if char.islower() else 0\n",
    "\n",
    "                feature_white = square * 12 + piece_id * 2 + color\n",
    "                feature_black = (square ^ 56) * 12 + piece_id * 2 + (1 - color)\n",
    "                features_white.append(feature_white)\n",
    "                features_black.append(feature_black)\n",
    "\n",
    "                file += 1\n",
    "                \n",
    "        return np.array(features_white), np.array(features_black)\n",
    "\n",
    "    @staticmethod\n",
    "    def fen_to_side_to_move(fen_str):\n",
    "        return 0 if fen_str.split()[1] == \"w\" else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNUE, self).__init__()\n",
    "\n",
    "        self.ft = nn.Linear(INPUT_SIZE, 128)\n",
    "        self.l1 = nn.Linear(2 * 128, 32)\n",
    "        self.l2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, white_features, black_features, stm):      \n",
    "        w = self.ft(white_features) # white's perspective\n",
    "        b = self.ft(black_features) # black's perspective\n",
    "        \n",
    "        # stm (side to move): 0 - white, 1 - black\n",
    "        accumulator = ((1 - stm) * torch.cat([w, b], dim=1)) + (stm * torch.cat([b, w], dim=1))\n",
    "        accumulator = torch.clamp(accumulator, 0.0, 1.0)\n",
    "\n",
    "        l1_out = self.l1(accumulator)\n",
    "        l1_out = torch.clamp(l1_out, 0.0, 1.0)\n",
    "\n",
    "        l2_out = self.l2(l1_out)\n",
    "        return l2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, name, lr, scaling_factor, optimizer, loss_fn, model_init = lambda: NNUE()):\n",
    "        self.name = name\n",
    "        self.model = model_init()\n",
    "        self.model.to(DEVICE)\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.optimizer = optimizer(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def start_run(self, run_name, num_batches):\n",
    "        self.model.train()\n",
    "        self.run_name = run_name\n",
    "        self.writer = SummaryWriter(f'runs/{run_name}_{self.name}')\n",
    "        self.running_loss = 0\n",
    "        self.best_validation_loss = 999999\n",
    "        self.num_batches = num_batches\n",
    "\n",
    "    def micro_batch(self, epoch, i, white_f, black_f, stm, true_eval):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        pred = self.model(white_f, black_f, stm)\n",
    "        pred = torch.tanh(pred / self.scaling_factor)\n",
    "        need = torch.tanh(true_eval / self.scaling_factor)\n",
    "\n",
    "        loss = self.loss_fn(pred, need)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.running_loss += loss.item()\n",
    "\n",
    "        if self.running_loss != self.running_loss:\n",
    "            print(white_f, black_f, stm, true_eval, pred)\n",
    "        \n",
    "        if i > 0 and i % REPORT_FREQ == 0 or i == self.num_batches - 1:\n",
    "            self.writer.add_scalar('training loss', self.running_loss / ((i - 1) % 500 + 1), epoch * self.num_batches + i)\n",
    "            self.running_loss = 0\n",
    "\n",
    "    # Validation run\n",
    "    def start_validation(self, epoch):\n",
    "        self.model.eval()\n",
    "        self.val_loss_scaled = 0\n",
    "        self.val_epoc = epoch\n",
    "        self.num_val_samples = 0\n",
    "    \n",
    "    def validation_batch(self, white_f, black_f, stm, true_eval):\n",
    "        pred = self.model(white_f, black_f, stm)\n",
    "        pred_scaled = torch.tanh(pred / self.scaling_factor)\n",
    "        need_scaled = torch.tanh(true_eval / self.scaling_factor)\n",
    "        self.val_loss_scaled += self.loss_fn(pred_scaled, need_scaled).item()\n",
    "        self.num_val_samples += 1\n",
    "    \n",
    "    def end_validation(self):\n",
    "        self.writer.add_scalar('validation loss', self.val_loss_scaled / self.num_val_samples, (self.val_epoc + 1) * self.num_batches)\n",
    "        self.model.train()\n",
    "\n",
    "        if self.val_loss_scaled < self.best_validation_loss:\n",
    "            save_dir = f\"checkpoints/{self.run_name}/{self.name}\"\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            \n",
    "            torch.save(self.model, f\"{save_dir}/epoch={str(self.val_epoc).zfill(3)},loss={round(self.val_loss_scaled / self.num_val_samples, 5)}\")\n",
    "\n",
    "            self.best_validation_loss = self.val_loss_scaled\n",
    "\n",
    "\n",
    "class MultiTrainer:\n",
    "    def __init__(self, trainers):\n",
    "        self.trainers = trainers\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        def forward_calls(*args, **kwargs):\n",
    "            for trainer in self.trainers:\n",
    "                getattr(trainer, name)(*args, **kwargs)\n",
    "        return forward_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainer, epochs, run_name, train_dl, validation_dl, epoch_start=0):\n",
    "    trainer.start_run(run_name, len(train_dl))\n",
    "\n",
    "    for epoch in tqdm(range(epoch_start, epochs), \"Training\"):\n",
    "        # Train\n",
    "        for i, batch in enumerate(tqdm((train_dl), f\"Epoch {epoch}\")):\n",
    "            white_f, black_f, stm, true_eval = (b.to(DEVICE) for b in batch)\n",
    "            trainer.micro_batch(epoch, i, white_f, black_f, stm, true_eval)\n",
    "      \n",
    "        \n",
    "        # Validate\n",
    "        trainer.start_validation(epoch)\n",
    "        for batch in tqdm(validation_dl, \"Computing validation loss\"):\n",
    "            white_f, black_f, stm, true_eval = (b.to(DEVICE) for b in batch)\n",
    "            trainer.validation_batch(white_f, black_f, stm, true_eval)\n",
    "            \n",
    "        trainer.end_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(run_name, override_train_size = None):\n",
    "    print(\"Loading dataset\")\n",
    "    dataset = FenDataset(DATASET_NAME, EVAL_COLUMN)\n",
    "\n",
    "    print(\"Preparing dataloaders\")\n",
    "    total_len = len(dataset) if override_train_size is None else int(override_train_size / TRAIN_SPLIT)\n",
    "    train_size = int(total_len * TRAIN_SPLIT)\n",
    "    validation_size = total_len - train_size\n",
    "    train_ds, validation_ds, _ = random_split(dataset, [train_size, validation_size, len(dataset) - train_size - validation_size])\n",
    "    train_dl = DataLoader(train_ds, batch_size=MICRO_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    validation_dl = DataLoader(validation_ds, batch_size=MICRO_BATCH_SIZE*10, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    print(\"Train size: \", train_size, \"validation size: \", validation_size)\n",
    "\n",
    "    print(\"Preparing trainers\")\n",
    "    trainers = [\n",
    "        Trainer(\"lr00085_sf35_adam\", 0.00085, 3.5, torch.optim.Adam, nn.MSELoss()),\n",
    "        Trainer(\"lr00085_sf55_adam\", 0.00085, 5.5, torch.optim.Adam, nn.MSELoss()),\n",
    "    ]\n",
    "    trainer = MultiTrainer(trainers)\n",
    "\n",
    "    print(\"Start training\")\n",
    "    train(trainer, TRAIN_EPOCHS, run_name, train_dl, validation_dl)\n",
    "\n",
    "    # del train_dl, validation_dl, dataset, train_ds, train_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    # (\"full_d8_nocap\", \"datasets/hce_full_train.csv\", \"eval_8\", None),\n",
    "    # (\"full_d8_cap\", \"datasets/hce_full_train.csv\", \"eval_8\", \"flags_8\"),\n",
    "    # (\"full_d5_nocap\", \"datasets/hce_full_train.csv\", \"eval_5\", None),\n",
    "    # (\"full_d5_cap\", \"datasets/hce_full_train.csv\", \"eval_5\", \"flags_5\"),\n",
    "    # (\"simple_d8_nocap\", \"datasets/train_simple_hce.csv\", \"eval_8\", None),\n",
    "    # (\"simple_d8_cap\", \"datasets/train_simple_hce.csv\", \"eval_8\", \"flags_8\"),\n",
    "    # (\"simple_d5_nocap\", \"datasets/train_simple_hce.csv\", \"eval_5\", None),\n",
    "    # (\"simplie_d5_cap\", \"datasets/train_simple_hce.csv\", \"eval_5\", \"flags_5\"),\n",
    "\n",
    "    #(\"full_d8_m2_norm\", \"datasets/hce_full_train.csv\", \"eval_8\", None, 2, 2300000),\n",
    "    #(\"full_d8_m4_norm\", \"datasets/hce_full_train.csv\", \"eval_8\", None, 4, 2300000),\n",
    "    # (\"full_d8_m6_norm\", \"datasets/hce_full_train.csv\", \"eval_8\", None, 6, 2300000),\n",
    "    # (\"full_d8_ma_norm\", \"datasets/hce_full_train.csv\", \"eval_8\", None, 100, 2300000),\n",
    "    # (\"full_d8_m0_norm\", \"datasets/hce_full_train.csv\", \"eval_8\", None, 0, 2300000),\n",
    "\n",
    "    # (\"full_d8_250k\", \"datasets/hce_full_train.csv\", \"eval_8\", None, 0, 250000),\n",
    "    # (\"full_d8_500k\", \"datasets/hce_full_train.csv\", \"eval_8\", None, 0, 500000),\n",
    "    # (\"full_d8_1000k\", \"datasets/hce_full_train.csv\", \"eval_8\", None, 0, 1000000),\n",
    "    # (\"full_d8_2000k\", \"datasets/hce_full_train.csv\", \"eval_8\", None, 0, 2000000),\n",
    "\n",
    "    # (\"full_d8_final\", \"datasets/hce_full_train.csv\", \"eval_8\", None, 4),\n",
    "    # (\"full_d5_final\", \"datasets/hce_full_train.csv\", \"eval_5\", None, 4),\n",
    "    (\"simple_d8_final\", \"datasets/train_simple_hce.csv\", \"eval_8\", None, 4),\n",
    "    # (\"simple_d5_final\", \"datasets/train_simple_hce.csv\", \"eval_5\", None, 4),\n",
    "]\n",
    "\n",
    "for p in params:\n",
    "    DATASET_NAME = p[1]\n",
    "    EVAL_COLUMN = p[2]\n",
    "    FLAG_COLUMN = p[3]\n",
    "    ACCEPTABLE_MATE_DEPTH = p[4]\n",
    "\n",
    "    if len(p) >= 6:\n",
    "        main(p[0], p[5])\n",
    "    else:\n",
    "        main(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model_path, fen):\n",
    "    model = torch.load(model_path)\n",
    "\n",
    "    fw, fb =  FenDataset.fen_to_features(fen)\n",
    "    stm = FenDataset.fen_to_side_to_move(fen)\n",
    "\n",
    "    white_features = np.zeros(INPUT_SIZE)\n",
    "    black_features = np.zeros(INPUT_SIZE)\n",
    "    white_features[fw] = 1\n",
    "    black_features[fb] = 1\n",
    "        \n",
    "    white_features = torch.tensor(white_features).unsqueeze(0).to(torch.float32).to(DEVICE)\n",
    "    black_features = torch.tensor(black_features).unsqueeze(0).to(torch.float32).to(DEVICE)\n",
    "    stm = torch.tensor(stm).unsqueeze(0).to(torch.float32).to(DEVICE)\n",
    "\n",
    "    return model(white_features, black_features, stm)\n",
    "\n",
    "# infer(\"/home/eff/bakalauras/models_es/d8_lr00085_sc35_simple_epoch_8_0_0815059666832288.torch\", \"rnbqkbnr/pppppppp/8/4N3/8/8/PPPPPPPP/RNBQKB1R b KQkq - 0 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_nnue(model_path, nnue_name):\n",
    "    def torch_get_weights(tensor_row):\n",
    "\t    return \"\\n\".join([str(tensor.item()) for tensor in tensor_row]) + \"\\n\"\n",
    "    \n",
    "    def torch_save_nn(model, filename):\n",
    "        print(\"Saving weights...\")\n",
    "        with open(filename, \"w\") as file:\n",
    "            for parameter in model.parameters():\n",
    "                if parameter.dim() == 1:\n",
    "                    print(\"B:\", len(parameter.data), \"x\", 1)\n",
    "                    row = parameter.data\n",
    "                    file.write(torch_get_weights(row))\n",
    "                elif parameter.dim() == 2:\n",
    "                    print(\"W:\", len(parameter.data), \"x\", len(parameter.data[0]))\n",
    "                    for row in parameter.data:\n",
    "                        file.write(torch_get_weights(row))\n",
    "                else:\n",
    "                    assert(0)\n",
    "        print(\"Weights saved\")\n",
    "\n",
    "    print(\"Saving\", model_path, \"as\", nnue_name)\n",
    "    model = torch.load(model_path).to(\"cpu\")\n",
    "    torch_save_nn(model, nnue_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_best_network(checkpoint_name, run_name):\n",
    "    models = os.listdir(f\"./checkpoints/{checkpoint_name}/{run_name}\")\n",
    "    models = sorted(models, key=lambda s: int(re.search(r\"epoch=(\\d+)\", s).group(1)), reverse=True)\n",
    "    return f\"./checkpoints/{checkpoint_name}/{run_name}/{models[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ./checkpoints/full_d8_final/lr00085_sf55_adam/epoch=038,loss=0.04115 as nets/D8_FULL.nnue\n",
      "Saving weights...\n",
      "W: 128 x 768\n",
      "B: 128 x 1\n",
      "W: 32 x 256\n",
      "B: 32 x 1\n",
      "W: 1 x 32\n",
      "B: 1 x 1\n",
      "Weights saved\n",
      "Saving ./checkpoints/full_d5_final/lr00085_sf55_adam/epoch=029,loss=0.03419 as nets/D5_FULL.nnue\n",
      "Saving weights...\n",
      "W: 128 x 768\n",
      "B: 128 x 1\n",
      "W: 32 x 256\n",
      "B: 32 x 1\n",
      "W: 1 x 32\n",
      "B: 1 x 1\n",
      "Weights saved\n",
      "Saving ./checkpoints/simple_d8_final/lr00085_sf55_adam/epoch=028,loss=0.03943 as nets/D8_SIMPLE.nnue\n",
      "Saving weights...\n",
      "W: 128 x 768\n",
      "B: 128 x 1\n",
      "W: 32 x 256\n",
      "B: 32 x 1\n",
      "W: 1 x 32\n",
      "B: 1 x 1\n",
      "Weights saved\n",
      "Saving ./checkpoints/simple_d5_final/lr00085_sf55_adam/epoch=030,loss=0.03598 as nets/D5_SIMPLE.nnue\n",
      "Saving weights...\n",
      "W: 128 x 768\n",
      "B: 128 x 1\n",
      "W: 32 x 256\n",
      "B: 32 x 1\n",
      "W: 1 x 32\n",
      "B: 1 x 1\n",
      "Weights saved\n"
     ]
    }
   ],
   "source": [
    "# export_nnue(get_best_network(\"full_d8_nocap\", \"lr00085_sf35_adam\"), \"nets/D8_FULL.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d8_nocap\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_55.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d8_cap\", \"lr00085_sf35_adam\"), \"nets/D8_FULL_NC.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d8_cap\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_NC_55.nnue\")\n",
    "\n",
    "# export_nnue(get_best_network(\"full_d5_nocap\", \"lr00085_sf35_adam\"), \"nets/D5_FULL.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d5_nocap\", \"lr00085_sf55_adam\"), \"nets/D5_FULL_55.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d5_cap\", \"lr00085_sf35_adam\"), \"nets/D5_FULL_NC.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d5_cap\", \"lr00085_sf55_adam\"), \"nets/D5_FULL_NC_55.nnue\")\n",
    "\n",
    "# export_nnue(get_best_network(\"simple_d8_nocap\", \"lr00085_sf35_adam\"), \"nets/D8_SIMPLE.nnue\")\n",
    "# export_nnue(get_best_network(\"simple_d8_nocap\", \"lr00085_sf55_adam\"), \"nets/D8_SIMPLE_55.nnue\")\n",
    "# export_nnue(get_best_network(\"simple_d8_cap\", \"lr00085_sf35_adam\"), \"nets/D8_SIMPLE_NC.nnue\")\n",
    "# export_nnue(get_best_network(\"simple_d8_cap\", \"lr00085_sf55_adam\"), \"nets/D8_SIMPLE_NC_55.nnue\")\n",
    "\n",
    "# export_nnue(get_best_network(\"simple_d5_nocap\", \"lr00085_sf35_adam\"), \"nets/D5_SIMPLE.nnue\")\n",
    "# export_nnue(get_best_network(\"simple_d5_nocap\", \"lr00085_sf55_adam\"), \"nets/D5_SIMPLE_55.nnue\")\n",
    "# export_nnue(get_best_network(\"simplie_d5_cap\", \"lr00085_sf35_adam\"), \"nets/D5_SIMPLE_NC.nnue\")\n",
    "# export_nnue(get_best_network(\"simplie_d5_cap\", \"lr00085_sf55_adam\"), \"nets/D5_SIMPLE_NC_55.nnue\")\n",
    "\n",
    "# export_nnue(get_best_network(\"full_d8_m2_norm\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_M2.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d8_m4_norm\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_M4.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d8_m6_norm\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_M6.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d8_ma_norm\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_MA.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d8_m0_norm\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_M0.nnue\")\n",
    "\n",
    "# export_nnue(get_best_network(\"full_d8_250k\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_250k.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d8_500k\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_500k.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d8_1000k\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_1000k.nnue\")\n",
    "# export_nnue(get_best_network(\"full_d8_2000k\", \"lr00085_sf55_adam\"), \"nets/D8_FULL_2000k.nnue\")\n",
    "\n",
    "export_nnue(get_best_network(\"full_d8_final\", \"lr00085_sf55_adam\"), \"nets/D8_FULL.nnue\")\n",
    "export_nnue(get_best_network(\"full_d5_final\", \"lr00085_sf55_adam\"), \"nets/D5_FULL.nnue\")\n",
    "export_nnue(get_best_network(\"simple_d8_final\", \"lr00085_sf55_adam\"), \"nets/D8_SIMPLE.nnue\")\n",
    "export_nnue(get_best_network(\"simple_d5_final\", \"lr00085_sf55_adam\"), \"nets/D5_SIMPLE.nnue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnues  = os.listdir(f\"./nets\")\n",
    "for nnue in nnues:\n",
    "    if not nnue.endswith('.nnue'):\n",
    "        continue\n",
    "    \n",
    "    name = nnue.split(\".\")[0]\n",
    "\n",
    "    pattern = \"\"\"{\n",
    "\t\t\"command\" : \"BoomChess.exe\",\n",
    "\t\t\"name\" : \"LatestBuild_NNUE_{NAME}\",\n",
    "\t\t\"options\" : [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"alias\" : \"\",\n",
    "\t\t\t\t\"default\" : 256,\n",
    "\t\t\t\t\"max\" : 1024,\n",
    "\t\t\t\t\"min\" : 1,\n",
    "\t\t\t\t\"name\" : \"Hash\",\n",
    "\t\t\t\t\"type\" : \"spin\",\n",
    "\t\t\t\t\"value\" : 256\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"alias\" : \"\",\n",
    "\t\t\t\t\"choices\" : [\n",
    "\t\t\t\t\t\"FULL\",\n",
    "\t\t\t\t\t\"SIMPLE\"\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"default\" : \"FULL\",\n",
    "\t\t\t\t\"name\" : \"EvalType\",\n",
    "\t\t\t\t\"type\" : \"combo\",\n",
    "\t\t\t\t\"value\" : \"SIMPLE\"\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"alias\" : \"\",\n",
    "\t\t\t\t\"default\" : \"<empty>\",\n",
    "\t\t\t\t\"name\" : \"NNUEPath\",\n",
    "\t\t\t\t\"type\" : \"folder\",\n",
    "\t\t\t\t\"value\" : \"C:/Users/marty/Desktop/Kursinis/nets/{NAME}.nnue\"\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t\t\"protocol\" : \"uci\",\n",
    "\t\t\"stderrFile\" : \"\",\n",
    "\t\t\"variants\" : [\n",
    "\t\t\t\"standard\",\n",
    "\t\t\t\"atomic\"\n",
    "\t\t],\n",
    "\t\t\"workingDirectory\" : \"C:\\\\\\\\Users\\\\\\\\marty\\\\\\\\Desktop\\\\\\\\Kursinis\\\\\\\\BoomChess\\\\\\\\cmake-build-release-mingw\"\n",
    "\t}\n",
    "    \"\"\"\n",
    "\n",
    "    print(pattern.replace(\"{NAME}\", name), \",\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
